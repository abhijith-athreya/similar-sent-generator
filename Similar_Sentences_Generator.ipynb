{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar sentences generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook tries to generate similar sentences to a given input sentence. \n",
    "- It makes use of WordNet and GloVe embeddings to arrive at substitute words for candidate words in a sentence.\n",
    "- Similarity threshold, and number of sentences generated can be controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Glove Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to glove vectors\n",
    "glove_path = \"../Word2vec/gensim_glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GloVe vectors\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the vocab\n",
    "vocab = glove_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the GloVe vectors\n",
    "len(vocab.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK + WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk is used to perform POS tagging\n",
    "import nltk \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility method to return \"simple\" POS tag for a given POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pos_identity(pos_tag):\n",
    "    \n",
    "    '''\n",
    "    This method returns\n",
    "    \n",
    "    1. 'np' for proper nouns, 'n' for all other nouns\n",
    "    \n",
    "    2. 'a' for adjectives\n",
    "    \n",
    "    3. 'v' for verbs\n",
    "    \n",
    "    4. 'r' for adverbs\n",
    "    \n",
    "    5. None for all other tags\n",
    "    '''\n",
    "    \n",
    "    if pos_tag in ['NNP', 'NNPS']:\n",
    "        return 'np'\n",
    "    elif pos_tag in ['NN', 'NNS']:\n",
    "        return 'n'\n",
    "    elif pos_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        return 'a'\n",
    "    elif pos_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return 'v'\n",
    "    elif pos_tag in ['RB', 'RBR', 'RBS']:\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(word, pos_tag, similarity_threshold):\n",
    "    \n",
    "    '''\n",
    "    This method returns most similar words to the word passed.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    word = input word\n",
    "    pos_tag = Simple POS tag of the word\n",
    "    similarity_threshold (float) = Value between 0 and 1. Indicates the similarity threshold to consider\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    a list of similar words, along with the original word\n",
    "    '''\n",
    "\n",
    "    # Lemmatize the word\n",
    "    word = lemmatizer.lemmatize(word, pos_tag)\n",
    "    # Get the synonyms and antonyms of a word\n",
    "    synonyms = [word] \n",
    "    #antonyms = [] \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        vector_check = glove_model.wv.get_vector(word)\n",
    "    except:\n",
    "        # If the word does not exist in the Glove model, return\n",
    "        return synonyms\n",
    "\n",
    "    for syn in wordnet.synsets(word): \n",
    "    \n",
    "        for l in syn.lemmas():\n",
    "        \n",
    "            try:\n",
    "            \n",
    "                if l.name() in synonyms:\n",
    "                    continue\n",
    "                \n",
    "                # Get the vector of the synonym\n",
    "                vector_prospect = glove_model.wv.get_vector(l.name())\n",
    "            \n",
    "                #print('Checking word = ', l.name())\n",
    "                cosine_diff = vocab.cosine_similarities(vector_1=vector_check, vectors_all=[vector_prospect])\n",
    "                #print(cosine_diff)\n",
    "            \n",
    "                #similar_by_vector()words_closer_than()n_similarity()\n",
    "                if cosine_diff > similarity_threshold:\n",
    "                    synonyms.append(l.name()) \n",
    "            \n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "        \n",
    "            #if l.antonyms(): \n",
    "             #   antonyms.append(l.antonyms()[0].name()) \n",
    "            \n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(total_synonym_array, position_array, last_position):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This method returns the next position of word replacement.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    total_synonym_array = Array containing the total length of synonyms\n",
    "    position_array = Array containing current positions\n",
    "    last_position_array = Integer\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    next position to be updated, -1 if all positions are exhausted\n",
    "    '''\n",
    "    new_pos = last_position\n",
    "    \n",
    "    for i in range(len(total_synonym_array)):\n",
    "        \n",
    "        # get a new position\n",
    "        new_pos = (new_pos + 1) % len(total_synonym_array)\n",
    "        \n",
    "        # if the new position is not good enough, fetch a new one\n",
    "        if position_array[new_pos] == -1 or position_array[new_pos] == total_synonym_array[new_pos]:\n",
    "            continue\n",
    "        else:\n",
    "            return new_pos\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_arrays(sentence_combination):\n",
    "    \n",
    "    '''\n",
    "    This is a utility method to get position arrays.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    sentence_combination = [[word], [word1, word2, ]]\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    two position arrays\n",
    "    '''\n",
    "    total_synonym_array = []\n",
    "    initial_position_array = []\n",
    "    \n",
    "    for each_word_array in sentence_combination:\n",
    "        length = len(each_word_array)\n",
    "        total_synonym_array.append(length)\n",
    "        if length == 1:\n",
    "            initial_position_array.append(-1)\n",
    "        else:\n",
    "            initial_position_array.append(0)\n",
    "    \n",
    "    return total_synonym_array, initial_position_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to provide an alternate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_alternate_sentence(sentence, num_versions=1, max_changes=1, similarity_threshold=0.7, ignore_stopwords=True, ignore_proper_nouns=True):\n",
    "    \n",
    "    '''\n",
    "    This method returns an alternate version(s) of the sentence passed by replacing words with their closest synonyms.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    sentence (String) = the input sentence\n",
    "    num_versions (int) = the number of alternate versions required\n",
    "    max_changes (int) = the maximum number of changes between versions\n",
    "    similarity_threshold (float) = Value between 0 and 1. Indicates the similarity threshold to consider while replacing words\n",
    "    ignore_stopwords (bool) = If True, stopwords will not be considered for replacement\n",
    "    ignore_proper_nouns (bool) = If True, proper nouns will be ignored for replacement\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    list of alternate sentence(s)\n",
    "    '''\n",
    "    \n",
    "    alternate_sentences = []\n",
    "    \n",
    "    sentence_combination = []\n",
    "    \n",
    "    # split the sentence into words\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # pos tag the sentence\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    \n",
    "    for each_word_pos in pos_tags:\n",
    "        \n",
    "        word = each_word_pos[0]\n",
    "        pos_tag = each_word_pos[1]\n",
    "        short_pos = fetch_pos_identity(pos_tag)\n",
    "        \n",
    "        # ignore proper nouns\n",
    "        if ignore_proper_nouns and 'np' == short_pos:\n",
    "            sentence_combination.append([word])\n",
    "            continue\n",
    "        \n",
    "        # lemmatize the word\n",
    "        if short_pos is not None:\n",
    "            word_lemmatized = lemmatizer.lemmatize(word, short_pos)\n",
    "        else:\n",
    "            word_lemmatized = lemmatizer.lemmatize(word)\n",
    "        \n",
    "        # ignore stopwords\n",
    "        if ignore_stopwords and (word_lemmatized in stop_words or word in stop_words):\n",
    "            sentence_combination.append([word])\n",
    "            continue\n",
    "        \n",
    "        # if POS is noun, adj, adv, or verb - get similar words\n",
    "        if short_pos is not None:\n",
    "            sentence_combination.append(get_related_words(word, short_pos, similarity_threshold))\n",
    "        # else do nothing\n",
    "        else:\n",
    "            sentence_combination.append([word])\n",
    "            continue\n",
    "    \n",
    "    total_synonym_array, position_array = get_position_arrays(sentence_combination)\n",
    "    \n",
    "    total_combos_possible = 0\n",
    "    for some_value in total_synonym_array:\n",
    "        if some_value > 1:\n",
    "            total_combos_possible = total_combos_possible + some_value\n",
    "    \n",
    "    total_combos_possible = total_combos_possible - 1\n",
    "    \n",
    "    last_position = -1\n",
    "    \n",
    " \n",
    "    for i in range(num_versions):\n",
    "        \n",
    "        if i >= total_combos_possible:\n",
    "            break\n",
    "        \n",
    "        # get the position to replace\n",
    "        position = get_next_position(total_synonym_array, position_array, last_position)\n",
    "        \n",
    "        #print(position)\n",
    "        \n",
    "        if position == -1:\n",
    "            break\n",
    "        \n",
    "        alt_sentence = ''\n",
    "        counter = 0\n",
    "        for j in sentence_combination:\n",
    "            alt_sentence = alt_sentence + ' '\n",
    "            if counter == position:\n",
    "                alt_sentence = alt_sentence + j[position_array[position]] # ] + 1 ]\n",
    "                position_array[position] = position_array[position] + 1\n",
    "                \n",
    "                last_position = position\n",
    "                \n",
    "            else:\n",
    "                if position_array[counter] > -1:\n",
    "                    alt_sentence = alt_sentence + j[position_array[counter] - 1]\n",
    "                else:\n",
    "                    alt_sentence = alt_sentence + j[position_array[counter]]\n",
    "                \n",
    "            \n",
    "            counter = counter + 1\n",
    "        \n",
    "        alt_sentence = alt_sentence.strip()\n",
    "        alternate_sentences.append(alt_sentence)\n",
    "            \n",
    "    return alternate_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provide_alternate_sentence('We collect your information regularly', num_versions=4, similarity_threshold=0.60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
